{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ebd211-43b3-45ba-8103-e7971a0a9d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#examine data file\n",
    "file_path = ''\n",
    "data = np.load(file_path, allow_pickle=True).item()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfc2d58-f737-471f-9e29-b7e10f9f6651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c138ca-e6a4-479b-92be-8f8bd1a11b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tcp_data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7b4a9adf-37c0-417f-8d29-301fab84d1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing D:/Filtered_Tasks\\task_0001_user_0012_scene_0009_cfg_0001\\transformed\\tcp.npy: 'NoneType' object is not subscriptable\n",
      "Error processing D:/Filtered_Tasks\\task_0100_user_0015_scene_0006_cfg_0001\\transformed\\tcp.npy: 'NoneType' object is not subscriptable\n",
      "Features saved to D:/statistical_features_tcp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\78087\\AppData\\Local\\Temp\\ipykernel_82844\\446210536.py:89: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_data = pd.concat([existing_data, new_data], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "def process_tcp_files(source_dir, target_csv):\n",
    "    \"\"\"\n",
    "    Extracts statistical features from all tcp.npy files in a directory structure and saves to a CSV.\n",
    "    \n",
    "    Parameters:\n",
    "    - source_dir: Path to the folder containing task folders.\n",
    "    - target_csv: Path to save the resulting CSV file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(target_csv):\n",
    "        with open(target_csv, 'w') as f:\n",
    "            f.write(\"task_num,\" + \",\".join([f\"{stat}_{feature}\" for feature in ['TCP_x', 'TCP_y', 'TCP_z', 'TCP_qx', 'TCP_qy', 'TCP_qz', 'TCP_qw'] \n",
    "                                           for stat in ['mean', 'std', 'min', 'max']]) + \"\\n\")\n",
    "\n",
    "    all_features = []\n",
    "    ct = 0\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        if \"transformed\" in root and \"tcp.npy\" in files:\n",
    "            ct += 1\n",
    "            try:\n",
    "                tcp_path = os.path.join(root, \"tcp.npy\")\n",
    "                #print(tcp_path)\n",
    "                data = np.load(tcp_path, allow_pickle=True).item()\n",
    "\n",
    "                extracted_data = []\n",
    "\n",
    "                for sensor_id, readings in data.items():\n",
    "                    for entry in readings:\n",
    "                        tcp = entry['tcp']\n",
    "                        robot_ft = entry['robot_ft']\n",
    "                        extracted_data.append({\n",
    "                            \"sensor_id\": sensor_id,\n",
    "                            \"timestamp\": entry['timestamp'],\n",
    "                            \"TCP_x\": tcp[0],\n",
    "                            \"TCP_y\": tcp[1],\n",
    "                            \"TCP_z\": tcp[2],\n",
    "                            \"TCP_qx\": tcp[3],\n",
    "                            \"TCP_qy\": tcp[4],\n",
    "                            \"TCP_qz\": tcp[5],\n",
    "                            \"TCP_qw\": tcp[6]\n",
    "                        })\n",
    "                \n",
    "\n",
    "\n",
    "                \n",
    "                df = pd.DataFrame(extracted_data)\n",
    "\n",
    "                stat_features = {}\n",
    "                for i, ft in enumerate(['TCP_x', 'TCP_y', 'TCP_z', 'TCP_qx', 'TCP_qy', 'TCP_qz','TCP_qw']):\n",
    "                    stat_features.update({\n",
    "                        f\"mean_{ft}\": df[ft].mean(),\n",
    "                        f\"std_{ft}\": df[ft].std(),\n",
    "                        f\"min_{ft}\": df[ft].min(),\n",
    "                        f\"max_{ft}\": df[ft].max(),\n",
    "                    })\n",
    "                #print(stat_features)\n",
    "\n",
    "                folder_name = root.split(os.sep)[-2]  \n",
    "                task_label = folder_name \n",
    "\n",
    "                row = [task_label] + list(stat_features.values())\n",
    "                all_features.append(row)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {tcp_path}: {e}\")\n",
    "        # if ct == 3:\n",
    "        #     break\n",
    "    header = [\"task_num\"] + [f\"{stat}_{feature}\" for feature in ['TCP_x', 'TCP_y', 'TCP_z', 'TCP_qx', 'TCP_qy', 'TCP_qz', 'TCP_qw'] \n",
    "                             for stat in ['mean', 'std', 'min', 'max']]\n",
    "\n",
    "    if os.path.exists(target_csv):\n",
    "        existing_data = pd.read_csv(target_csv)\n",
    "        new_data = pd.DataFrame(all_features, columns=header)\n",
    "        final_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "    else:\n",
    "        final_data = pd.DataFrame(all_features, columns=header)\n",
    "\n",
    "    final_data.to_csv(target_csv, index=False)\n",
    "    print(f\"Features saved to {target_csv}\")\n",
    "\n",
    "source_directory = \"D:/Filtered_Tasks\" \n",
    "target_csv_path = r\"D:/statistical_features_tcp_dropsampling.csv\"\n",
    "process_tcp_files(source_directory, target_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95bd68f-3b65-4bae-9f67-8559fcd91d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a04584-bf97-453f-815d-f20edd641aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c525d0b-7ac5-4c56-a2a5-9463bfa06bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "99e6b8d5-a4e5-455b-9532-b24a9103b28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing D:/Filtered_Tasks\\task_0001_user_0012_scene_0009_cfg_0001\\transformed\\force_torque.npy: 'NoneType' object is not subscriptable\n",
      "Error processing D:/Filtered_Tasks\\task_0100_user_0015_scene_0006_cfg_0001\\transformed\\force_torque.npy: 'NoneType' object is not subscriptable\n",
      "Features saved to D:/statistical_features_force_torque.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\78087\\AppData\\Local\\Temp\\ipykernel_82844\\843215869.py:100: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_data = pd.concat([existing_data, new_data], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "def process_force_torque_files(source_dir, target_csv):\n",
    "    \"\"\"\n",
    "    Extracts statistical features from all force_torque.npy files and saves them to a CSV.\n",
    "\n",
    "    Parameters:\n",
    "    - source_dir: Path to the folder containing task folders.\n",
    "    - target_csv: Path to save the resulting CSV file.\n",
    "    \"\"\"\n",
    "    # Check if CSV exists, if not create it with a header\n",
    "    if not os.path.exists(target_csv):\n",
    "        with open(target_csv, 'w') as f:\n",
    "            header = [\"task_num\"] + [f\"{stat}_{ft}\" for ft in ['Fx', 'Fy', 'Fz', 'Tx', 'Ty', 'Tz'] \n",
    "                                     for stat in ['mean', 'std', 'min', 'max']]\n",
    "            f.write(\",\".join(header) + \"\\n\")\n",
    "\n",
    "    # List to hold rows for the CSV\n",
    "    all_features = []\n",
    "\n",
    "    # Walk through all task folders\n",
    "    ct = 0\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        #print(root)\n",
    "        if \"transformed\" in root and \"force_torque.npy\" in files:\n",
    "            #ct += 1\n",
    "            try:\n",
    "                # Load force_torque.npy file\n",
    "                ft_path = os.path.join(root, \"force_torque.npy\")\n",
    "                data = np.load(ft_path, allow_pickle=True).item()\n",
    "\n",
    "                extracted_data = []\n",
    "                for key, readings in data.items():\n",
    "                    for entry in readings:\n",
    "                        extracted_data.append({\n",
    "                            \"sensor_id\": key,\n",
    "                            \"timestamp\": entry['timestamp'],\n",
    "                            \"Fx\": entry['zeroed'][0],\n",
    "                            \"Fy\": entry['zeroed'][1],\n",
    "                            \"Fz\": entry['zeroed'][2],\n",
    "                            \"Tx\": entry['zeroed'][3],\n",
    "                            \"Ty\": entry['zeroed'][4],\n",
    "                            \"Tz\": entry['zeroed'][5],\n",
    "                        })\n",
    "                df = pd.DataFrame(extracted_data)\n",
    "                #print(df.head())\n",
    "                # Extract and concatenate all 'zeroed' force/torque values\n",
    "                # zeroed_values = []\n",
    "                # for timestamp, values in data.items():\n",
    "                #     for entry in values:\n",
    "                #         # Validate 'zeroed' data\n",
    "                #         if 'zeroed' in entry and len(entry['zeroed']) == 6:\n",
    "                #             zeroed_values.append(entry['zeroed'])\n",
    "                \n",
    "                # Check for empty data\n",
    "                # if len(zeroed_values) == 0:\n",
    "                #     print(f\"No valid data in {ft_path}. Skipping.\")\n",
    "                #     continue\n",
    "                \n",
    "                # # Convert to NumPy array\n",
    "                # ft_array = np.array(zeroed_values)\n",
    "                \n",
    "                # Compute statistical features\n",
    "                stat_features = {}\n",
    "                for i, ft in enumerate(['Fx', 'Fy', 'Fz', 'Tx', 'Ty', 'Tz']):\n",
    "                    stat_features.update({\n",
    "                        f\"mean_{ft}\": df[ft].mean(),\n",
    "                        f\"std_{ft}\": df[ft].std(),\n",
    "                        f\"min_{ft}\": df[ft].min(),\n",
    "                        f\"max_{ft}\": df[ft].max(),\n",
    "                    })\n",
    "                    #print(df[ft].mean(), df[ft].std(), df[ft].min(), df[ft].max())\n",
    "\n",
    "\n",
    "                # Extract task_num from folder name\n",
    "                folder_name = root.split(os.sep)[-2]  # Parent folder of 'transformed'\n",
    "                task_label = folder_name  # Use the full folder name as the label\n",
    "\n",
    "                # Add task_label as the label instead of task_num\n",
    "                row = [task_label] + list(stat_features.values())\n",
    "                #print(f'current row is: {row}')\n",
    "\n",
    "                # Append row to the list\n",
    "                all_features.append(row)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {ft_path}: {e}\")\n",
    "\n",
    "\n",
    "    # Save all features to the CSV\n",
    "    header = [\"task_num\"] + [f\"{stat}_{ft}\" for ft in ['Fx', 'Fy', 'Fz', 'Tx', 'Ty', 'Tz'] \n",
    "                             for stat in ['mean', 'std', 'min', 'max']]\n",
    "\n",
    "    # If file already exists, append rows\n",
    "    if os.path.exists(target_csv):\n",
    "        existing_data = pd.read_csv(target_csv)\n",
    "        new_data = pd.DataFrame(all_features, columns=header)\n",
    "        final_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "    else:\n",
    "        final_data = pd.DataFrame(all_features, columns=header)\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    final_data.to_csv(target_csv, index=False)\n",
    "    print(f\"Features saved to {target_csv}\")\n",
    "\n",
    "# Define source and target directories\n",
    "source_directory = \"D:/Filtered_Tasks\"  # Update with your path\n",
    "target_csv_path = \"D:/statistical_features_force_torque.csv\"\n",
    "\n",
    "# Run the pipeline\n",
    "process_force_torque_files(source_directory, target_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e758b45-eabb-4136-a82f-c16c8f6104c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fce153-4376-4650-8150-bca895b28805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Down-sampling for reduced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "402e85f0-85cd-433c-b50c-fe85c437e020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing D:/Filtered_Tasks\\task_0001_user_0012_scene_0009_cfg_0001\\transformed\\tcp.npy: 'NoneType' object is not subscriptable\n",
      "Error processing D:/Filtered_Tasks\\task_0100_user_0015_scene_0006_cfg_0001\\transformed\\tcp.npy: 'NoneType' object is not subscriptable\n",
      "Features saved to D:/statistical_features_tcp_dropsampling.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\78087\\AppData\\Local\\Temp\\ipykernel_165548\\3279215482.py:101: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_data = pd.concat([existing_data, new_data], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "def process_tcp_files_wdownsampling(source_dir, target_csv, downsample_rate = 3):\n",
    "    \"\"\"\n",
    "    Extracts statistical features from all tcp.npy files in a directory structure and saves to a CSV.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(target_csv):\n",
    "        with open(target_csv, 'w') as f:\n",
    "            f.write(\"task_num,\" + \",\".join([f\"{stat}_{feature}\" for feature in ['TCP_x', 'TCP_y', 'TCP_z', 'TCP_qx', 'TCP_qy', 'TCP_qz', 'TCP_qw'] \n",
    "                                           for stat in ['mean', 'std', 'min', 'max']]) + \"\\n\")\n",
    "\n",
    "    all_features = []\n",
    "    ct = 0\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        if \"transformed\" in root and \"tcp.npy\" in files:\n",
    "            ct += 1\n",
    "            try:\n",
    "                tcp_path = os.path.join(root, \"tcp.npy\")\n",
    "                #print(tcp_path)\n",
    "                data = np.load(tcp_path, allow_pickle=True).item()\n",
    "\n",
    "                extracted_data = []\n",
    "\n",
    "                for sensor_id, readings in data.items():\n",
    "                    for entry in readings:\n",
    "                        tcp = entry['tcp']\n",
    "                        robot_ft = entry['robot_ft']\n",
    "                        extracted_data.append({\n",
    "                            \"sensor_id\": sensor_id,\n",
    "                            \"timestamp\": entry['timestamp'],\n",
    "                            \"TCP_x\": tcp[0],\n",
    "                            \"TCP_y\": tcp[1],\n",
    "                            \"TCP_z\": tcp[2],\n",
    "                            \"TCP_qx\": tcp[3],\n",
    "                            \"TCP_qy\": tcp[4],\n",
    "                            \"TCP_qz\": tcp[5],\n",
    "                            \"TCP_qw\": tcp[6]\n",
    "                        })\n",
    "                \n",
    "\n",
    "\n",
    "                \n",
    "                df = pd.DataFrame(extracted_data)\n",
    "                #print(f'Raw dataframe before downsampling size: {df.shape}')\n",
    "                #print(df.head())\n",
    "\n",
    "                df_downsampled = df.iloc[::downsample_rate, :].reset_index(drop=True)\n",
    "\n",
    "                if len(df_downsampled) < 3: \n",
    "                    print(f\"Skipping {tcp_path} due to insufficient data after downsampling.\")\n",
    "                    continue\n",
    "                    \n",
    "                #print(f'Down-sampled dataframe before downsampling size: {df_downsampled.shape}')\n",
    "                #print(df_downsampled.head())\n",
    "\n",
    "                stat_features = {}\n",
    "                for i, ft in enumerate(['TCP_x', 'TCP_y', 'TCP_z', 'TCP_qx', 'TCP_qy', 'TCP_qz','TCP_qw']):\n",
    "                    stat_features.update({\n",
    "                        f\"mean_{ft}\": df_downsampled[ft].mean(),\n",
    "                        f\"std_{ft}\": df_downsampled[ft].std(),\n",
    "                        f\"min_{ft}\": df_downsampled[ft].min(),\n",
    "                        f\"max_{ft}\": df_downsampled[ft].max(),\n",
    "                    })\n",
    "\n",
    "                folder_name = root.split(os.sep)[-2]  \n",
    "                task_label = folder_name \n",
    "\n",
    "                row = [task_label] + list(stat_features.values())\n",
    "\n",
    "                all_features.append(row)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {tcp_path}: {e}\")\n",
    "        # if ct == 1:\n",
    "        #     break\n",
    "    header = [\"task_num\"] + [f\"{stat}_{feature}\" for feature in ['TCP_x', 'TCP_y', 'TCP_z', 'TCP_qx', 'TCP_qy', 'TCP_qz', 'TCP_qw'] \n",
    "                             for stat in ['mean', 'std', 'min', 'max']]\n",
    "\n",
    "    if os.path.exists(target_csv):\n",
    "        existing_data = pd.read_csv(target_csv)\n",
    "        new_data = pd.DataFrame(all_features, columns=header)\n",
    "        #print(new_data)\n",
    "        final_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "        #print(final_data)\n",
    "    else:\n",
    "        final_data = pd.DataFrame(all_features, columns=header)\n",
    "\n",
    "    final_data.to_csv(target_csv, index=False)\n",
    "    print(f\"Features saved to {target_csv}\")\n",
    "\n",
    "source_directory = \"D:/Filtered_Tasks\" \n",
    "target_csv_path = r\"D:/statistical_features_tcp_dropsampling.csv\"\n",
    "\n",
    "process_tcp_files_wdownsampling(source_directory, target_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a39747-8d28-418c-a98c-a8d8a00b4f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9bbe1f03-356a-4025-9a5e-edc2f17656ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing D:/Filtered_Tasks\\task_0001_user_0012_scene_0009_cfg_0001\\transformed\\force_torque.npy: 'NoneType' object is not subscriptable\n",
      "Error processing D:/Filtered_Tasks\\task_0100_user_0015_scene_0006_cfg_0001\\transformed\\force_torque.npy: 'NoneType' object is not subscriptable\n",
      "Features saved to D:/statistical_features_force_torque_downsampling.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\78087\\AppData\\Local\\Temp\\ipykernel_165548\\4022597207.py:103: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_data = pd.concat([existing_data, new_data], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "def process_force_torque_files_wdownsampling(source_dir, target_csv, downsample_rate = 3):\n",
    "    \"\"\"\n",
    "    Extracts statistical features from all force_torque.npy files and saves them to a CSV.\n",
    "    \"\"\"\n",
    "    # Check if CSV exists, if not create it with a header\n",
    "    if not os.path.exists(target_csv):\n",
    "        with open(target_csv, 'w') as f:\n",
    "            header = [\"task_num\"] + [f\"{stat}_{ft}\" for ft in ['Fx', 'Fy', 'Fz', 'Tx', 'Ty', 'Tz'] \n",
    "                                     for stat in ['mean', 'std', 'min', 'max']]\n",
    "            f.write(\",\".join(header) + \"\\n\")\n",
    "    all_features = []\n",
    "    \n",
    "    ct = 0\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        #print(root)\n",
    "        if \"transformed\" in root and \"force_torque.npy\" in files:\n",
    "            ct += 1\n",
    "            try:\n",
    "                ft_path = os.path.join(root, \"force_torque.npy\")\n",
    "                data = np.load(ft_path, allow_pickle=True).item()\n",
    "\n",
    "                extracted_data = []\n",
    "                for key, readings in data.items():\n",
    "                    for entry in readings:\n",
    "                        extracted_data.append({\n",
    "                            \"sensor_id\": key,\n",
    "                            \"timestamp\": entry['timestamp'],\n",
    "                            \"Fx\": entry['zeroed'][0],\n",
    "                            \"Fy\": entry['zeroed'][1],\n",
    "                            \"Fz\": entry['zeroed'][2],\n",
    "                            \"Tx\": entry['zeroed'][3],\n",
    "                            \"Ty\": entry['zeroed'][4],\n",
    "                            \"Tz\": entry['zeroed'][5],\n",
    "                        })\n",
    "                df = pd.DataFrame(extracted_data)\n",
    "\n",
    "                df_downsampled = df.iloc[::downsample_rate, :].reset_index(drop=True)\n",
    "                #print(df.head())\n",
    "                # zeroed_values = []\n",
    "                # for timestamp, values in data.items():\n",
    "                #     for entry in values:\n",
    "                #         if 'zeroed' in entry and len(entry['zeroed']) == 6:\n",
    "                #             zeroed_values.append(entry['zeroed'])\n",
    "                \n",
    "                # if len(zeroed_values) == 0:\n",
    "                #     print(f\"No valid data in {ft_path}. Skipping.\")\n",
    "                #     continue\n",
    "                \n",
    "                # ft_array = np.array(zeroed_values)\n",
    "                \n",
    "                # Compute statistical features\n",
    "                stat_features = {}\n",
    "                for i, ft in enumerate(['Fx', 'Fy', 'Fz', 'Tx', 'Ty', 'Tz']):\n",
    "                    stat_features.update({\n",
    "                        f\"mean_{ft}\": df_downsampled[ft].mean(),\n",
    "                        f\"std_{ft}\": df_downsampled[ft].std(),\n",
    "                        f\"min_{ft}\": df_downsampled[ft].min(),\n",
    "                        f\"max_{ft}\": df_downsampled[ft].max(),\n",
    "                    })\n",
    "                    #print(df[ft].mean(), df[ft].std(), df[ft].min(), df[ft].max())\n",
    "\n",
    "\n",
    "                folder_name = root.split(os.sep)[-2] \n",
    "                task_label = folder_name\n",
    "                row = [task_label] + list(stat_features.values())\n",
    "                #print(f'current row is: {row}')\n",
    "\n",
    "                # Append row to the list\n",
    "                all_features.append(row)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {ft_path}: {e}\")\n",
    "\n",
    "        # if ct == 1:\n",
    "        #     break\n",
    "    header = [\"task_num\"] + [f\"{stat}_{ft}\" for ft in ['Fx', 'Fy', 'Fz', 'Tx', 'Ty', 'Tz'] \n",
    "                             for stat in ['mean', 'std', 'min', 'max']]\n",
    "\n",
    "    if os.path.exists(target_csv):\n",
    "        existing_data = pd.read_csv(target_csv)\n",
    "        new_data = pd.DataFrame(all_features, columns=header)\n",
    "        final_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "    else:\n",
    "        final_data = pd.DataFrame(all_features, columns=header)\n",
    "\n",
    "    # Save the DataFrame to CSV\n",
    "    final_data.to_csv(target_csv, index=False)\n",
    "    print(f\"Features saved to {target_csv}\")\n",
    "\n",
    "source_directory = \"D:/Filtered_Tasks\"\n",
    "target_csv_path = \"D:/statistical_features_force_torque_downsampling.csv\"\n",
    "\n",
    "process_force_torque_files_wdownsampling(source_directory, target_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcc7ea5-5beb-4637-a8f4-1801d8719023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "603a046d-0510-4c87-b897-608d82f6c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4cf02817-3291-4302-9ec0-a0cb42ce7226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing D:/Filtered_Tasks\\task_0001_user_0012_scene_0009_cfg_0001\\transformed\\tcp.npy: 'NoneType' object is not subscriptable\n",
      "Error processing D:/Filtered_Tasks\\task_0100_user_0015_scene_0006_cfg_0001\\transformed\\tcp.npy: 'NoneType' object is not subscriptable\n",
      "Features saved to D:/statistical_features_tcp_aggregation.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\78087\\AppData\\Local\\Temp\\ipykernel_165548\\3184171483.py:103: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_data = pd.concat([existing_data, new_data], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "def process_tcp_files_waggreagation(source_dir, target_csv, agg_window = 3):\n",
    "    \"\"\"\n",
    "    Extracts statistical features from all tcp.npy files in a directory structure and saves to a CSV.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(target_csv):\n",
    "        with open(target_csv, 'w') as f:\n",
    "            f.write(\"task_num,\" + \",\".join([f\"{stat}_{feature}\" for feature in ['TCP_x', 'TCP_y', 'TCP_z', 'TCP_qx', 'TCP_qy', 'TCP_qz', 'TCP_qw'] \n",
    "                                           for stat in ['mean', 'std', 'min', 'max']]) + \"\\n\")\n",
    "\n",
    "    all_features = []\n",
    "    ct = 0\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        if \"transformed\" in root and \"tcp.npy\" in files:\n",
    "            ct += 1\n",
    "            try:\n",
    "                tcp_path = os.path.join(root, \"tcp.npy\")\n",
    "                #print(tcp_path)\n",
    "                data = np.load(tcp_path, allow_pickle=True).item()\n",
    "\n",
    "                extracted_data = []\n",
    "\n",
    "                for sensor_id, readings in data.items():\n",
    "                    for entry in readings:\n",
    "                        tcp = entry['tcp']\n",
    "                        robot_ft = entry['robot_ft']\n",
    "                        extracted_data.append({\n",
    "                            \"sensor_id\": sensor_id,\n",
    "                            \"timestamp\": entry['timestamp'],\n",
    "                            \"TCP_x\": tcp[0],\n",
    "                            \"TCP_y\": tcp[1],\n",
    "                            \"TCP_z\": tcp[2],\n",
    "                            \"TCP_qx\": tcp[3],\n",
    "                            \"TCP_qy\": tcp[4],\n",
    "                            \"TCP_qz\": tcp[5],\n",
    "                            \"TCP_qw\": tcp[6]\n",
    "                        })\n",
    "                \n",
    "\n",
    "\n",
    "                \n",
    "                df = pd.DataFrame(extracted_data)\n",
    "                #print(f'Raw dataframe before downsampling size: {df.shape}')\n",
    "                #print(df.head(6))\n",
    "\n",
    "                df_filtered = df[['TCP_x', \"TCP_y\", \"TCP_z\", \"TCP_qx\", \"TCP_qy\", \"TCP_qz\", \"TCP_qw\"]]\n",
    "                df_aggregated = df_filtered.groupby(df_filtered.index // agg_window).mean()\n",
    "\n",
    "                if len(df_aggregated) < 3:\n",
    "                    print(f\"Skipping {tcp_path} due to insufficient data after aggregation.\")\n",
    "                    continue\n",
    "                    \n",
    "                #print(f'Aggregated dataframe before downsampling size: {df_aggregated.shape}')\n",
    "                #print(df_aggregated.head())\n",
    "\n",
    "                stat_features = {}\n",
    "                for i, ft in enumerate(['TCP_x', 'TCP_y', 'TCP_z', 'TCP_qx', 'TCP_qy', 'TCP_qz','TCP_qw']):\n",
    "                    stat_features.update({\n",
    "                        f\"mean_{ft}\": df_aggregated[ft].mean(),\n",
    "                        f\"std_{ft}\": df_aggregated[ft].std(),\n",
    "                        f\"min_{ft}\": df_aggregated[ft].min(),\n",
    "                        f\"max_{ft}\": df_aggregated[ft].max(),\n",
    "                    })\n",
    "                #print(stat_features)\n",
    "\n",
    "                folder_name = root.split(os.sep)[-2] \n",
    "                task_label = folder_name\n",
    "\n",
    "                row = [task_label] + list(stat_features.values())\n",
    "\n",
    "                all_features.append(row)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {tcp_path}: {e}\")\n",
    "        #if ct == 1:\n",
    "        #    break\n",
    "    header = [\"task_num\"] + [f\"{stat}_{feature}\" for feature in ['TCP_x', 'TCP_y', 'TCP_z', 'TCP_qx', 'TCP_qy', 'TCP_qz', 'TCP_qw'] \n",
    "                             for stat in ['mean', 'std', 'min', 'max']]\n",
    "\n",
    "    if os.path.exists(target_csv):\n",
    "        existing_data = pd.read_csv(target_csv)\n",
    "        new_data = pd.DataFrame(all_features, columns=header)\n",
    "        #print(new_data)\n",
    "        final_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "        #print(final_data)\n",
    "    else:\n",
    "        final_data = pd.DataFrame(all_features, columns=header)\n",
    "\n",
    "    final_data.to_csv(target_csv, index=False)\n",
    "    print(f\"Features saved to {target_csv}\")\n",
    "\n",
    "source_directory = \"D:/Filtered_Tasks\" \n",
    "target_csv_path = r\"D:/statistical_features_tcp_aggregation.csv\"\n",
    "\n",
    "process_tcp_files_waggreagation(source_directory, target_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6177c5-6fbd-46b3-90a2-428d366cb244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "72fded1f-47d3-49f0-a0e9-d93b92e2e2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing D:/Filtered_Tasks\\task_0001_user_0012_scene_0009_cfg_0001\\transformed\\force_torque.npy: 'NoneType' object is not subscriptable\n",
      "Error processing D:/Filtered_Tasks\\task_0100_user_0015_scene_0006_cfg_0001\\transformed\\force_torque.npy: 'NoneType' object is not subscriptable\n",
      "Features saved to D:/statistical_features_force_torque_aggregation.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\78087\\AppData\\Local\\Temp\\ipykernel_165548\\2258408637.py:112: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_data = pd.concat([existing_data, new_data], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "def process_force_torque_files_waggregation(source_dir, target_csv, agg_window = 3):\n",
    "    \"\"\"\n",
    "    Extracts statistical features from all force_torque.npy files and saves them to a CSV.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(target_csv):\n",
    "        with open(target_csv, 'w') as f:\n",
    "            header = [\"task_num\"] + [f\"{stat}_{ft}\" for ft in ['Fx', 'Fy', 'Fz', 'Tx', 'Ty', 'Tz'] \n",
    "                                     for stat in ['mean', 'std', 'min', 'max']]\n",
    "            f.write(\",\".join(header) + \"\\n\")\n",
    "\n",
    "    all_features = []\n",
    "\n",
    "    ct = 0\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        #print(root)\n",
    "        if \"transformed\" in root and \"force_torque.npy\" in files:\n",
    "            ct += 1\n",
    "            try:\n",
    "                ft_path = os.path.join(root, \"force_torque.npy\")\n",
    "                data = np.load(ft_path, allow_pickle=True).item()\n",
    "\n",
    "                extracted_data = []\n",
    "                for key, readings in data.items():\n",
    "                    for entry in readings:\n",
    "                        extracted_data.append({\n",
    "                            \"sensor_id\": key,\n",
    "                            \"timestamp\": entry['timestamp'],\n",
    "                            \"Fx\": entry['zeroed'][0],\n",
    "                            \"Fy\": entry['zeroed'][1],\n",
    "                            \"Fz\": entry['zeroed'][2],\n",
    "                            \"Tx\": entry['zeroed'][3],\n",
    "                            \"Ty\": entry['zeroed'][4],\n",
    "                            \"Tz\": entry['zeroed'][5],\n",
    "                        })\n",
    "                df = pd.DataFrame(extracted_data)\n",
    "\n",
    "                df_filtered = df[['Fx', \"Fy\", \"Fz\", \"Tx\", \"Ty\", \"Tz\"]]\n",
    "                df_aggregated = df_filtered.groupby(df_filtered.index // agg_window).mean()\n",
    "\n",
    "\n",
    "                if len(df_aggregated) < 3: \n",
    "                    print(f\"Skipping {tcp_path} due to insufficient data after aggregation.\")\n",
    "                    continue\n",
    "\n",
    "                #df_downsampled = df.iloc[::downsample_rate, :].reset_index(drop=True)\n",
    "                #print(df.head(6))\n",
    "                #print(df_aggregated.head())\n",
    "                # zeroed_values = []\n",
    "                # for timestamp, values in data.items():\n",
    "                #     for entry in values:\n",
    "                #         if 'zeroed' in entry and len(entry['zeroed']) == 6:\n",
    "                #             zeroed_values.append(entry['zeroed'])\n",
    "                \n",
    "                # if len(zeroed_values) == 0:\n",
    "                #     print(f\"No valid data in {ft_path}. Skipping.\")\n",
    "                #     continue\n",
    "                \n",
    "                # ft_array = np.array(zeroed_values)\n",
    "                \n",
    "                stat_features = {}\n",
    "                for i, ft in enumerate(['Fx', 'Fy', 'Fz', 'Tx', 'Ty', 'Tz']):\n",
    "                    stat_features.update({\n",
    "                        f\"mean_{ft}\": df_aggregated[ft].mean(),\n",
    "                        f\"std_{ft}\": df_aggregated[ft].std(),\n",
    "                        f\"min_{ft}\": df_aggregated[ft].min(),\n",
    "                        f\"max_{ft}\": df_aggregated[ft].max(),\n",
    "                    })\n",
    "                    #print(df[ft].mean(), df[ft].std(), df[ft].min(), df[ft].max())\n",
    "\n",
    "\n",
    "                folder_name = root.split(os.sep)[-2] \n",
    "                task_label = folder_name \n",
    "\n",
    "                row = [task_label] + list(stat_features.values())\n",
    "                #print(f'current row is: {row}')\n",
    "\n",
    "                all_features.append(row)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {ft_path}: {e}\")\n",
    "\n",
    "        # if ct == 1:\n",
    "        #     break\n",
    "    header = [\"task_num\"] + [f\"{stat}_{ft}\" for ft in ['Fx', 'Fy', 'Fz', 'Tx', 'Ty', 'Tz'] \n",
    "                             for stat in ['mean', 'std', 'min', 'max']]\n",
    "\n",
    "    if os.path.exists(target_csv):\n",
    "        existing_data = pd.read_csv(target_csv)\n",
    "        new_data = pd.DataFrame(all_features, columns=header)\n",
    "        final_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "    else:\n",
    "        final_data = pd.DataFrame(all_features, columns=header)\n",
    "\n",
    "    final_data.to_csv(target_csv, index=False)\n",
    "    print(f\"Features saved to {target_csv}\")\n",
    "\n",
    "source_directory = \"D:/Filtered_Tasks\" \n",
    "target_csv_path = \"D:/statistical_features_force_torque_aggregation.csv\"\n",
    "\n",
    "# Run the pipeline\n",
    "process_force_torque_files_waggregation(source_directory, target_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89df348e-1f87-4a47-8c61-cd31914065da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
