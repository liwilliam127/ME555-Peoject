{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "9ea16cb4-1419-4659-99e1-0738a9072c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a604b53b-1e81-4064-8071-56ac28e561c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"VAE_Gaussian_augmented_dataset.csv\")\n",
    "#test_df = pd.read_csv(\"test.csv\")\n",
    "# print(test_df.shape)\n",
    "# print(test_df.isnull().values.any())\n",
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3a35c1b-ea8f-4c17-8b69-f2174740b361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9288, 53)\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_Fx</th>\n",
       "      <th>std_Fx</th>\n",
       "      <th>min_Fx</th>\n",
       "      <th>max_Fx</th>\n",
       "      <th>mean_Fy</th>\n",
       "      <th>std_Fy</th>\n",
       "      <th>min_Fy</th>\n",
       "      <th>max_Fy</th>\n",
       "      <th>mean_Fz</th>\n",
       "      <th>std_Fz</th>\n",
       "      <th>...</th>\n",
       "      <th>max_TCP_qy</th>\n",
       "      <th>mean_TCP_qz</th>\n",
       "      <th>std_TCP_qz</th>\n",
       "      <th>min_TCP_qz</th>\n",
       "      <th>max_TCP_qz</th>\n",
       "      <th>mean_TCP_qw</th>\n",
       "      <th>std_TCP_qw</th>\n",
       "      <th>min_TCP_qw</th>\n",
       "      <th>max_TCP_qw</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.091094</td>\n",
       "      <td>0.551535</td>\n",
       "      <td>-2.716940</td>\n",
       "      <td>5.026344</td>\n",
       "      <td>-0.389788</td>\n",
       "      <td>1.482441</td>\n",
       "      <td>-14.920257</td>\n",
       "      <td>2.075711</td>\n",
       "      <td>-0.551167</td>\n",
       "      <td>1.995361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114844</td>\n",
       "      <td>-0.067169</td>\n",
       "      <td>0.210216</td>\n",
       "      <td>-0.397239</td>\n",
       "      <td>0.328366</td>\n",
       "      <td>0.007217</td>\n",
       "      <td>0.715606</td>\n",
       "      <td>-0.983862</td>\n",
       "      <td>0.992033</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.042475</td>\n",
       "      <td>0.558085</td>\n",
       "      <td>-4.307300</td>\n",
       "      <td>8.625980</td>\n",
       "      <td>-0.271580</td>\n",
       "      <td>1.426492</td>\n",
       "      <td>-16.193495</td>\n",
       "      <td>2.097393</td>\n",
       "      <td>-0.349012</td>\n",
       "      <td>1.884769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080914</td>\n",
       "      <td>-0.065886</td>\n",
       "      <td>0.201699</td>\n",
       "      <td>-0.372874</td>\n",
       "      <td>0.344875</td>\n",
       "      <td>-0.050501</td>\n",
       "      <td>0.713410</td>\n",
       "      <td>-0.989799</td>\n",
       "      <td>0.990807</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.061610</td>\n",
       "      <td>0.624525</td>\n",
       "      <td>-4.413770</td>\n",
       "      <td>10.329532</td>\n",
       "      <td>-0.310435</td>\n",
       "      <td>2.127228</td>\n",
       "      <td>-28.516469</td>\n",
       "      <td>2.023712</td>\n",
       "      <td>-0.410629</td>\n",
       "      <td>2.892710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081576</td>\n",
       "      <td>-0.076213</td>\n",
       "      <td>0.194011</td>\n",
       "      <td>-0.370489</td>\n",
       "      <td>0.328109</td>\n",
       "      <td>-0.129140</td>\n",
       "      <td>0.704608</td>\n",
       "      <td>-0.992530</td>\n",
       "      <td>0.987084</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.108036</td>\n",
       "      <td>0.733039</td>\n",
       "      <td>-6.900487</td>\n",
       "      <td>8.984035</td>\n",
       "      <td>-0.593093</td>\n",
       "      <td>2.149323</td>\n",
       "      <td>-21.772929</td>\n",
       "      <td>2.335922</td>\n",
       "      <td>-0.784609</td>\n",
       "      <td>2.876551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006726</td>\n",
       "      <td>-0.084439</td>\n",
       "      <td>0.195364</td>\n",
       "      <td>-0.416887</td>\n",
       "      <td>0.328214</td>\n",
       "      <td>-0.166065</td>\n",
       "      <td>0.686092</td>\n",
       "      <td>-0.996298</td>\n",
       "      <td>0.921216</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.048892</td>\n",
       "      <td>0.503397</td>\n",
       "      <td>-3.754972</td>\n",
       "      <td>4.944276</td>\n",
       "      <td>-0.260897</td>\n",
       "      <td>1.332355</td>\n",
       "      <td>-15.793961</td>\n",
       "      <td>1.640494</td>\n",
       "      <td>-0.354656</td>\n",
       "      <td>1.820410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091534</td>\n",
       "      <td>-0.070010</td>\n",
       "      <td>0.221369</td>\n",
       "      <td>-0.406463</td>\n",
       "      <td>0.413101</td>\n",
       "      <td>-0.041900</td>\n",
       "      <td>0.711247</td>\n",
       "      <td>-0.994797</td>\n",
       "      <td>0.986257</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_Fx    std_Fx    min_Fx     max_Fx   mean_Fy    std_Fy     min_Fy  \\\n",
       "0  0.091094  0.551535 -2.716940   5.026344 -0.389788  1.482441 -14.920257   \n",
       "1  0.042475  0.558085 -4.307300   8.625980 -0.271580  1.426492 -16.193495   \n",
       "2  0.061610  0.624525 -4.413770  10.329532 -0.310435  2.127228 -28.516469   \n",
       "3  0.108036  0.733039 -6.900487   8.984035 -0.593093  2.149323 -21.772929   \n",
       "4  0.048892  0.503397 -3.754972   4.944276 -0.260897  1.332355 -15.793961   \n",
       "\n",
       "     max_Fy   mean_Fz    std_Fz  ...  max_TCP_qy  mean_TCP_qz  std_TCP_qz  \\\n",
       "0  2.075711 -0.551167  1.995361  ...    0.114844    -0.067169    0.210216   \n",
       "1  2.097393 -0.349012  1.884769  ...    0.080914    -0.065886    0.201699   \n",
       "2  2.023712 -0.410629  2.892710  ...    0.081576    -0.076213    0.194011   \n",
       "3  2.335922 -0.784609  2.876551  ...    0.006726    -0.084439    0.195364   \n",
       "4  1.640494 -0.354656  1.820410  ...    0.091534    -0.070010    0.221369   \n",
       "\n",
       "   min_TCP_qz  max_TCP_qz  mean_TCP_qw  std_TCP_qw  min_TCP_qw  max_TCP_qw  \\\n",
       "0   -0.397239    0.328366     0.007217    0.715606   -0.983862    0.992033   \n",
       "1   -0.372874    0.344875    -0.050501    0.713410   -0.989799    0.990807   \n",
       "2   -0.370489    0.328109    -0.129140    0.704608   -0.992530    0.987084   \n",
       "3   -0.416887    0.328214    -0.166065    0.686092   -0.996298    0.921216   \n",
       "4   -0.406463    0.413101    -0.041900    0.711247   -0.994797    0.986257   \n",
       "\n",
       "   class  \n",
       "0      4  \n",
       "1      4  \n",
       "2      4  \n",
       "3      4  \n",
       "4      4  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv(\"VAE_Gaussian_augmented_dataset.csv\")\n",
    "print(df.shape)\n",
    "print(df.isnull().values.any())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f37a85-91eb-4e85-b50e-4e6aac9dbafa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca3972-77e9-481b-94cd-ed7ff1a14af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "638d011b-9bfc-43af-a4bc-ba571e917f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"class\"])  # Features: all columns except the label\n",
    "y = df[\"class\"]  # Class labels\n",
    "\n",
    "# Encode the labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Convert labels to one-hot encoding for categorical cross-entropy\n",
    "y_onehot = to_categorical(y_encoded)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape the data for 1D CNN (samples, timesteps, features)\n",
    "# Timesteps = 1 since we are treating each task as one sequence\n",
    "X_reshaped = X_scaled[..., np.newaxis]  # Adding 1 channel for CNN input\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_reshaped, y_onehot, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5553df54-ea0f-4579-b85e-07abd758acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1], 1)),  # Explicit input layer\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(y_onehot.shape[1], activation='softmax')  # Output layer with softmax for multi-class\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32c1347d-5270-4f74-89d5-77eee8c0b9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">90,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │           \u001b[38;5;34m128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m6,208\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m704\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m90,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m516\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">97,092</span> (379.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m97,092\u001b[0m (379.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">97,092</span> (379.27 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m97,092\u001b[0m (379.27 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0037981-6747-4d80-91bf-e5be25bac2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6071 - loss: 0.9601 - val_accuracy: 0.8520 - val_loss: 0.4097\n",
      "Epoch 2/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8337 - loss: 0.4795 - val_accuracy: 0.8794 - val_loss: 0.3281\n",
      "Epoch 3/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8713 - loss: 0.3735 - val_accuracy: 0.8977 - val_loss: 0.2797\n",
      "Epoch 4/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8775 - loss: 0.3467 - val_accuracy: 0.9026 - val_loss: 0.2478\n",
      "Epoch 5/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8920 - loss: 0.2795 - val_accuracy: 0.9160 - val_loss: 0.2256\n",
      "Epoch 6/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9006 - loss: 0.2759 - val_accuracy: 0.9257 - val_loss: 0.1993\n",
      "Epoch 7/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9050 - loss: 0.2544 - val_accuracy: 0.9279 - val_loss: 0.1992\n",
      "Epoch 8/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9109 - loss: 0.2385 - val_accuracy: 0.9354 - val_loss: 0.1776\n",
      "Epoch 9/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9208 - loss: 0.2180 - val_accuracy: 0.9376 - val_loss: 0.1872\n",
      "Epoch 10/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9241 - loss: 0.1997 - val_accuracy: 0.9440 - val_loss: 0.1657\n",
      "Epoch 11/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9226 - loss: 0.1931 - val_accuracy: 0.9462 - val_loss: 0.1530\n",
      "Epoch 12/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9282 - loss: 0.1879 - val_accuracy: 0.9483 - val_loss: 0.1492\n",
      "Epoch 13/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9336 - loss: 0.1816 - val_accuracy: 0.9494 - val_loss: 0.1481\n",
      "Epoch 14/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9355 - loss: 0.1678 - val_accuracy: 0.9591 - val_loss: 0.1306\n",
      "Epoch 15/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9374 - loss: 0.1655 - val_accuracy: 0.9499 - val_loss: 0.1440\n",
      "Epoch 16/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9394 - loss: 0.1655 - val_accuracy: 0.9602 - val_loss: 0.1215\n",
      "Epoch 17/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9382 - loss: 0.1739 - val_accuracy: 0.9543 - val_loss: 0.1361\n",
      "Epoch 18/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9426 - loss: 0.1570 - val_accuracy: 0.9559 - val_loss: 0.1233\n",
      "Epoch 19/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9484 - loss: 0.1596 - val_accuracy: 0.9645 - val_loss: 0.1147\n",
      "Epoch 20/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9468 - loss: 0.1392 - val_accuracy: 0.9607 - val_loss: 0.1148\n",
      "Epoch 21/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9439 - loss: 0.1462 - val_accuracy: 0.9602 - val_loss: 0.1143\n",
      "Epoch 22/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9504 - loss: 0.1387 - val_accuracy: 0.9607 - val_loss: 0.1144\n",
      "Epoch 23/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9535 - loss: 0.1230 - val_accuracy: 0.9612 - val_loss: 0.1260\n",
      "Epoch 24/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9490 - loss: 0.1369 - val_accuracy: 0.9645 - val_loss: 0.1093\n",
      "Epoch 25/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9546 - loss: 0.1277 - val_accuracy: 0.9634 - val_loss: 0.1023\n",
      "Epoch 26/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9536 - loss: 0.1275 - val_accuracy: 0.9688 - val_loss: 0.0994\n",
      "Epoch 27/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9614 - loss: 0.1053 - val_accuracy: 0.9693 - val_loss: 0.0962\n",
      "Epoch 28/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9601 - loss: 0.1132 - val_accuracy: 0.9682 - val_loss: 0.0968\n",
      "Epoch 29/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9587 - loss: 0.1280 - val_accuracy: 0.9704 - val_loss: 0.0973\n",
      "Epoch 30/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9609 - loss: 0.1033 - val_accuracy: 0.9709 - val_loss: 0.0931\n",
      "Epoch 31/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9587 - loss: 0.1171 - val_accuracy: 0.9682 - val_loss: 0.0976\n",
      "Epoch 32/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9572 - loss: 0.1185 - val_accuracy: 0.9682 - val_loss: 0.0929\n",
      "Epoch 33/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9628 - loss: 0.1031 - val_accuracy: 0.9726 - val_loss: 0.0833\n",
      "Epoch 34/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9597 - loss: 0.1112 - val_accuracy: 0.9715 - val_loss: 0.0863\n",
      "Epoch 35/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9610 - loss: 0.1077 - val_accuracy: 0.9736 - val_loss: 0.0824\n",
      "Epoch 36/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9660 - loss: 0.0993 - val_accuracy: 0.9747 - val_loss: 0.0767\n",
      "Epoch 37/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9620 - loss: 0.0944 - val_accuracy: 0.9774 - val_loss: 0.0746\n",
      "Epoch 38/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9663 - loss: 0.1008 - val_accuracy: 0.9758 - val_loss: 0.0739\n",
      "Epoch 39/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9692 - loss: 0.0871 - val_accuracy: 0.9763 - val_loss: 0.0839\n",
      "Epoch 40/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9677 - loss: 0.0930 - val_accuracy: 0.9769 - val_loss: 0.0784\n",
      "Epoch 41/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9701 - loss: 0.0792 - val_accuracy: 0.9742 - val_loss: 0.0775\n",
      "Epoch 42/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9666 - loss: 0.0877 - val_accuracy: 0.9801 - val_loss: 0.0732\n",
      "Epoch 43/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9695 - loss: 0.0815 - val_accuracy: 0.9779 - val_loss: 0.0765\n",
      "Epoch 44/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9682 - loss: 0.0990 - val_accuracy: 0.9822 - val_loss: 0.0741\n",
      "Epoch 45/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9720 - loss: 0.0801 - val_accuracy: 0.9752 - val_loss: 0.0835\n",
      "Epoch 46/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9686 - loss: 0.0848 - val_accuracy: 0.9774 - val_loss: 0.0768\n",
      "Epoch 47/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9739 - loss: 0.0763 - val_accuracy: 0.9806 - val_loss: 0.0789\n",
      "Epoch 48/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9674 - loss: 0.0850 - val_accuracy: 0.9801 - val_loss: 0.0715\n",
      "Epoch 49/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9678 - loss: 0.0904 - val_accuracy: 0.9839 - val_loss: 0.0684\n",
      "Epoch 50/50\n",
      "\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9670 - loss: 0.0871 - val_accuracy: 0.9828 - val_loss: 0.0595\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 889us/step - accuracy: 0.9820 - loss: 0.0537\n",
      "Test Loss: 0.0595\n",
      "Test Accuracy: 0.9828\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4912c929-3705-422a-b7ff-08515ad008ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9d6661-c2ff-4298-bd64-07355ad5120a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "f3c9db83-b577-4261-958a-32940ef9efbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5030, 53)\n",
      "(129, 53)\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_Fx</th>\n",
       "      <th>std_Fx</th>\n",
       "      <th>min_Fx</th>\n",
       "      <th>max_Fx</th>\n",
       "      <th>mean_Fy</th>\n",
       "      <th>std_Fy</th>\n",
       "      <th>min_Fy</th>\n",
       "      <th>max_Fy</th>\n",
       "      <th>mean_Fz</th>\n",
       "      <th>std_Fz</th>\n",
       "      <th>...</th>\n",
       "      <th>max_TCP_qy</th>\n",
       "      <th>mean_TCP_qz</th>\n",
       "      <th>std_TCP_qz</th>\n",
       "      <th>min_TCP_qz</th>\n",
       "      <th>max_TCP_qz</th>\n",
       "      <th>mean_TCP_qw</th>\n",
       "      <th>std_TCP_qw</th>\n",
       "      <th>min_TCP_qw</th>\n",
       "      <th>max_TCP_qw</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011254</td>\n",
       "      <td>0.442999</td>\n",
       "      <td>-1.920936</td>\n",
       "      <td>2.064664</td>\n",
       "      <td>-0.312191</td>\n",
       "      <td>1.293969</td>\n",
       "      <td>-9.808524</td>\n",
       "      <td>1.843434</td>\n",
       "      <td>-0.411665</td>\n",
       "      <td>1.499849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047622</td>\n",
       "      <td>-0.085202</td>\n",
       "      <td>0.245813</td>\n",
       "      <td>-0.479852</td>\n",
       "      <td>0.368453</td>\n",
       "      <td>-0.113048</td>\n",
       "      <td>0.671682</td>\n",
       "      <td>-0.979342</td>\n",
       "      <td>0.979416</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.028499</td>\n",
       "      <td>0.619903</td>\n",
       "      <td>-3.646468</td>\n",
       "      <td>4.933733</td>\n",
       "      <td>-0.399350</td>\n",
       "      <td>1.676785</td>\n",
       "      <td>-13.139836</td>\n",
       "      <td>2.169235</td>\n",
       "      <td>-0.526530</td>\n",
       "      <td>1.987745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184625</td>\n",
       "      <td>0.026310</td>\n",
       "      <td>0.243701</td>\n",
       "      <td>-0.526289</td>\n",
       "      <td>0.518053</td>\n",
       "      <td>0.144180</td>\n",
       "      <td>0.707204</td>\n",
       "      <td>-0.981018</td>\n",
       "      <td>0.981118</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.027236</td>\n",
       "      <td>1.018709</td>\n",
       "      <td>-3.262223</td>\n",
       "      <td>3.485886</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.744631</td>\n",
       "      <td>-2.493998</td>\n",
       "      <td>3.280509</td>\n",
       "      <td>0.224399</td>\n",
       "      <td>0.712818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268659</td>\n",
       "      <td>0.006792</td>\n",
       "      <td>0.174839</td>\n",
       "      <td>-0.486699</td>\n",
       "      <td>0.333708</td>\n",
       "      <td>-0.119803</td>\n",
       "      <td>0.700712</td>\n",
       "      <td>-0.994174</td>\n",
       "      <td>0.995862</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.023499</td>\n",
       "      <td>2.058170</td>\n",
       "      <td>-8.664618</td>\n",
       "      <td>8.899791</td>\n",
       "      <td>0.280381</td>\n",
       "      <td>1.923219</td>\n",
       "      <td>-8.294284</td>\n",
       "      <td>8.722299</td>\n",
       "      <td>0.450387</td>\n",
       "      <td>1.639716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100171</td>\n",
       "      <td>-0.063907</td>\n",
       "      <td>0.142825</td>\n",
       "      <td>-0.437442</td>\n",
       "      <td>0.327301</td>\n",
       "      <td>0.054084</td>\n",
       "      <td>0.737196</td>\n",
       "      <td>-0.994185</td>\n",
       "      <td>0.999513</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.051871</td>\n",
       "      <td>0.493422</td>\n",
       "      <td>-2.070790</td>\n",
       "      <td>2.273246</td>\n",
       "      <td>-0.156795</td>\n",
       "      <td>0.486445</td>\n",
       "      <td>-5.142684</td>\n",
       "      <td>1.491303</td>\n",
       "      <td>-0.199192</td>\n",
       "      <td>0.494531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061934</td>\n",
       "      <td>-0.058214</td>\n",
       "      <td>0.204305</td>\n",
       "      <td>-0.435687</td>\n",
       "      <td>0.335504</td>\n",
       "      <td>-0.071536</td>\n",
       "      <td>0.680499</td>\n",
       "      <td>-0.991251</td>\n",
       "      <td>0.989002</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_Fx    std_Fx    min_Fx    max_Fx   mean_Fy    std_Fy     min_Fy  \\\n",
       "0  0.011254  0.442999 -1.920936  2.064664 -0.312191  1.293969  -9.808524   \n",
       "1 -0.028499  0.619903 -3.646468  4.933733 -0.399350  1.676785 -13.139836   \n",
       "2  0.027236  1.018709 -3.262223  3.485886  0.002927  0.744631  -2.493998   \n",
       "3  0.023499  2.058170 -8.664618  8.899791  0.280381  1.923219  -8.294284   \n",
       "4  0.051871  0.493422 -2.070790  2.273246 -0.156795  0.486445  -5.142684   \n",
       "\n",
       "     max_Fy   mean_Fz    std_Fz  ...  max_TCP_qy  mean_TCP_qz  std_TCP_qz  \\\n",
       "0  1.843434 -0.411665  1.499849  ...    0.047622    -0.085202    0.245813   \n",
       "1  2.169235 -0.526530  1.987745  ...    0.184625     0.026310    0.243701   \n",
       "2  3.280509  0.224399  0.712818  ...    0.268659     0.006792    0.174839   \n",
       "3  8.722299  0.450387  1.639716  ...    0.100171    -0.063907    0.142825   \n",
       "4  1.491303 -0.199192  0.494531  ...    0.061934    -0.058214    0.204305   \n",
       "\n",
       "   min_TCP_qz  max_TCP_qz  mean_TCP_qw  std_TCP_qw  min_TCP_qw  max_TCP_qw  \\\n",
       "0   -0.479852    0.368453    -0.113048    0.671682   -0.979342    0.979416   \n",
       "1   -0.526289    0.518053     0.144180    0.707204   -0.981018    0.981118   \n",
       "2   -0.486699    0.333708    -0.119803    0.700712   -0.994174    0.995862   \n",
       "3   -0.437442    0.327301     0.054084    0.737196   -0.994185    0.999513   \n",
       "4   -0.435687    0.335504    -0.071536    0.680499   -0.991251    0.989002   \n",
       "\n",
       "   class  \n",
       "0      4  \n",
       "1      4  \n",
       "2      3  \n",
       "3      2  \n",
       "4      1  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"VAE_Gaussian_augmented_train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "print(df.shape)\n",
    "print(test_df.shape)\n",
    "print(test_df.isnull().values.any())\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "b1093f74-c248-42b9-8420-94bd30849a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"class\"])  # Features: all columns except the label\n",
    "y = df[\"class\"]  # Class labels\n",
    "\n",
    "# Encode the labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Convert labels to one-hot encoding for categorical cross-entropy\n",
    "y_onehot = to_categorical(y_encoded)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape the data for 1D CNN (samples, timesteps, features)\n",
    "# Timesteps = 1 since we are treating each task as one sequence\n",
    "X_reshaped = X_scaled[..., np.newaxis]  # Adding 1 channel for CNN input\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X_reshaped, y_onehot, test_size=0.2, random_state=42\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "8b31fea1-96e0-4185-8157-1395ad57bbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total models tested: 96\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6038 - loss: 1.5573\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5832 - loss: 1.0280 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5264 - loss: 1.5700 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5870 - loss: 1.0823 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6700 - loss: 1.2811 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6125 - loss: 1.0210\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5805 - loss: 1.7235\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5745 - loss: 1.0712 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6346 - loss: 1.0075 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5714 - loss: 1.1933 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6493 - loss: 1.1321 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6017 - loss: 1.0091 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5727 - loss: 1.7536 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5554 - loss: 1.1410 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6103 - loss: 1.6085 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6056 - loss: 1.0428 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6229 - loss: 1.6052 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5437 - loss: 1.2733\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6480 - loss: 1.5463 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6142 - loss: 1.1351 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5727 - loss: 1.5977 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6160 - loss: 1.0136\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6104 - loss: 2.2529 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6562 - loss: 0.8965 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6355 - loss: 1.3021 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5766 - loss: 1.1074 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6186 - loss: 1.8893 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6217 - loss: 1.1896\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6541 - loss: 1.9345 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5619 - loss: 1.2869 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6082 - loss: 2.2713 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5406 - loss: 1.6212\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5934 - loss: 1.4930 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5875 - loss: 1.1631 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6562 - loss: 1.1455 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5749 - loss: 1.2020 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6411 - loss: 1.1602 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5381 - loss: 1.1588 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5692 - loss: 2.0155 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5714 - loss: 0.9866 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6268 - loss: 1.3947\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5636 - loss: 1.1003\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6593 - loss: 1.9309 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5957 - loss: 1.0861 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6109 - loss: 1.8621 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5774 - loss: 1.1204 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5948 - loss: 2.1761\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5995 - loss: 1.1896 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5830 - loss: 1.9634 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5974 - loss: 1.1681 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6298 - loss: 1.6692 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5411 - loss: 1.2891 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6338 - loss: 2.0602 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6017 - loss: 1.2756 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6082 - loss: 2.7960 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5905 - loss: 1.3256\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5813 - loss: 2.3386 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5571 - loss: 1.5139 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6619 - loss: 1.7111 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5792 - loss: 1.5476 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5831 - loss: 2.6593 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5870 - loss: 1.6533 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6359 - loss: 2.5454 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5861 - loss: 1.7588 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5099 - loss: 2.0370 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5220 - loss: 1.2298 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6060 - loss: 1.6572 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5480 - loss: 1.2861 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5437 - loss: 2.3661 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5657 - loss: 1.1748 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5627 - loss: 2.1943 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6039 - loss: 1.2645 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5467 - loss: 2.3470 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6001 - loss: 1.2091 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6047 - loss: 2.4616 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5779 - loss: 1.3801\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5952 - loss: 2.6365 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6026 - loss: 1.1834 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6870 - loss: 1.8165 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5930 - loss: 1.0760 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6069 - loss: 2.9283 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5827 - loss: 1.3927 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5861 - loss: 2.3434 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6372 - loss: 1.1588 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6381 - loss: 2.4313 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6346 - loss: 1.4386\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5965 - loss: 2.6144\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5943 - loss: 1.4962 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6290 - loss: 2.6502 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5610 - loss: 1.3178 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6004 - loss: 2.9339 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5901 - loss: 1.5032 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5138 - loss: 4.6838 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5683 - loss: 1.7332 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5909 - loss: 3.5350 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6060 - loss: 1.9604 \n",
      "Done!!\n",
      "highest test accuracy achieved: 0.7054263353347778\n"
     ]
    }
   ],
   "source": [
    "filter_sizes = [16, 32, 64]\n",
    "filter2_sizes = [16, 32, 64, 128]\n",
    "denses = [16, 32, 64, 128]\n",
    "kernel_sizes = [3]\n",
    "pool_sizes = [2,3]\n",
    "batch_sizes = [32]\n",
    "print(f'Total models tested: {3*4*4*1*2*1}')\n",
    "ct = 0\n",
    "acc_list = []\n",
    "for f1 in filter_sizes:\n",
    "    for f2 in filter2_sizes:\n",
    "        for d in denses:\n",
    "            for ks in kernel_sizes:\n",
    "                for ps in pool_sizes:\n",
    "                    for bs in batch_sizes:\n",
    "                        ct+=1\n",
    "                        model = Sequential([\n",
    "                            Input(shape=(X_train.shape[1], 1)),  # Explicit input layer\n",
    "                            Conv1D(filters=f1, kernel_size=ks, activation='relu'),\n",
    "                            MaxPooling1D(pool_size=ps),\n",
    "                            Dropout(0.2),\n",
    "                            \n",
    "                            Conv1D(filters=f2, kernel_size=ks, activation='relu'),\n",
    "                            MaxPooling1D(pool_size=ps),\n",
    "                            Dropout(0.2),\n",
    "                            \n",
    "                            Flatten(),\n",
    "                            Dense(d, activation='relu'),\n",
    "                            Dropout(0.3),\n",
    "                            Dense(y_onehot.shape[1], activation='softmax')  # Output layer with softmax for multi-class\n",
    "                        ])\n",
    "                        \n",
    "                        model.compile(optimizer='adam',\n",
    "                                      loss='categorical_crossentropy',\n",
    "                                      metrics=['accuracy'])\n",
    "                        \n",
    "                        history = model.fit(\n",
    "                            X_reshaped, y_onehot,\n",
    "                            epochs=25,\n",
    "                            batch_size=bs,\n",
    "                            validation_data=(X_train, y_train),\n",
    "                            verbose=0\n",
    "                        )\n",
    "\n",
    "                        testset_loss, testset_accuracy = model.evaluate(X_test_reshaped, y_test_onehot, verbose=1)\n",
    "                        acc_list.append(([f1, f2, d, ks, ps, bs], testset_accuracy))\n",
    "                        if ct%100 ==0:\n",
    "                            print(f\"the {ct}th model training completed\")\n",
    "print('Done!!')\n",
    "print(f'highest test accuracy achieved: {max([i[1] for i in acc_list])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "df4fc9b6-919e-46e1-8d9a-18f35d3e8dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([16, 16, 64, 3, 2, 32], 0.7054263353347778),\n",
       " ([32, 16, 32, 3, 2, 32], 0.6899224519729614),\n",
       " ([64, 32, 128, 3, 2, 32], 0.6899224519729614),\n",
       " ([16, 64, 128, 3, 3, 32], 0.6744186282157898),\n",
       " ([32, 128, 32, 3, 2, 32], 0.6744186282157898),\n",
       " ([16, 32, 32, 3, 2, 32], 0.6666666865348816),\n",
       " ([16, 64, 32, 3, 2, 32], 0.6666666865348816),\n",
       " ([32, 16, 64, 3, 2, 32], 0.6666666865348816),\n",
       " ([16, 32, 16, 3, 2, 32], 0.6589147448539734),\n",
       " ([16, 128, 64, 3, 2, 32], 0.6589147448539734)]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(acc_list, key=lambda x: x[1], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e62f8b-b313-4c93-b083-65695e6f4fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a1333c-0f9c-4790-bd37-50466562709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testset = test_df.drop(columns=[\"class\"])  # Features: all columns except the label\n",
    "y_testset = test_df[\"class\"]  # Class labels\n",
    "\n",
    "# Encode the labels to integers\n",
    "#label_encoder = LabelEncoder()\n",
    "y_test_encoded = label_encoder.fit_transform(y_testset)\n",
    "\n",
    "# Convert labels to one-hot encoding for categorical cross-entropy\n",
    "y_test_onehot = to_categorical(y_test_encoded)\n",
    "\n",
    "# Normalize the features\n",
    "#scaler = StandardScaler()\n",
    "X_test_scaled = scaler.fit_transform(X_testset)\n",
    "\n",
    "# Reshape the data for 1D CNN (samples, timesteps, features)\n",
    "# Timesteps = 1 since we are treating each task as one sequence\n",
    "X_test_reshaped = X_test_scaled[..., np.newaxis]  # Adding 1 channel for CNN input\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "# test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "# print(f\"Test Loss: {test_loss:.4f}\")\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "58068473-dcd3-4ba0-9b81-1cc4b104ed3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - F1Score: 0.5520 - accuracy: 0.5519 - loss: 1.0669 - val_F1Score: 0.5625 - val_accuracy: 0.5736 - val_loss: 1.1215\n",
      "Epoch 2/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - F1Score: 0.7713 - accuracy: 0.7679 - loss: 0.6865 - val_F1Score: 0.5903 - val_accuracy: 0.6202 - val_loss: 1.0153\n",
      "Epoch 3/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - F1Score: 0.7892 - accuracy: 0.7859 - loss: 0.5786 - val_F1Score: 0.6455 - val_accuracy: 0.6667 - val_loss: 0.9077\n",
      "Epoch 4/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - F1Score: 0.8134 - accuracy: 0.8122 - loss: 0.5871 - val_F1Score: 0.6672 - val_accuracy: 0.6822 - val_loss: 0.9019\n",
      "Epoch 5/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - F1Score: 0.8398 - accuracy: 0.8364 - loss: 0.4538 - val_F1Score: 0.6384 - val_accuracy: 0.6512 - val_loss: 0.9352\n",
      "Epoch 6/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - F1Score: 0.8430 - accuracy: 0.8413 - loss: 0.4391 - val_F1Score: 0.6706 - val_accuracy: 0.6822 - val_loss: 0.8073\n",
      "Epoch 7/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - F1Score: 0.8422 - accuracy: 0.8378 - loss: 0.4297 - val_F1Score: 0.6832 - val_accuracy: 0.6899 - val_loss: 0.8260\n",
      "Epoch 8/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - F1Score: 0.8529 - accuracy: 0.8481 - loss: 0.4067 - val_F1Score: 0.6299 - val_accuracy: 0.6434 - val_loss: 0.9367\n",
      "Epoch 9/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - F1Score: 0.8682 - accuracy: 0.8646 - loss: 0.3701 - val_F1Score: 0.6628 - val_accuracy: 0.6822 - val_loss: 0.9137\n",
      "Epoch 10/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - F1Score: 0.8537 - accuracy: 0.8524 - loss: 0.3783 - val_F1Score: 0.6587 - val_accuracy: 0.6589 - val_loss: 0.8915\n",
      "Epoch 11/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - F1Score: 0.8689 - accuracy: 0.8659 - loss: 0.3601 - val_F1Score: 0.6588 - val_accuracy: 0.6667 - val_loss: 0.9819\n",
      "Epoch 12/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - F1Score: 0.8664 - accuracy: 0.8639 - loss: 0.3523 - val_F1Score: 0.6705 - val_accuracy: 0.6822 - val_loss: 0.9211\n",
      "Epoch 13/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - F1Score: 0.8785 - accuracy: 0.8746 - loss: 0.3243 - val_F1Score: 0.6530 - val_accuracy: 0.6512 - val_loss: 0.9636\n",
      "Epoch 14/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - F1Score: 0.8847 - accuracy: 0.8820 - loss: 0.3110 - val_F1Score: 0.6968 - val_accuracy: 0.6977 - val_loss: 0.9689\n",
      "Epoch 15/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - F1Score: 0.8806 - accuracy: 0.8796 - loss: 0.3371 - val_F1Score: 0.6686 - val_accuracy: 0.6744 - val_loss: 0.9582\n",
      "Epoch 16/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - F1Score: 0.8848 - accuracy: 0.8822 - loss: 0.3220 - val_F1Score: 0.6685 - val_accuracy: 0.6667 - val_loss: 0.9643\n",
      "Epoch 17/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - F1Score: 0.8868 - accuracy: 0.8852 - loss: 0.3103 - val_F1Score: 0.6395 - val_accuracy: 0.6512 - val_loss: 0.9492\n",
      "Epoch 18/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - F1Score: 0.8803 - accuracy: 0.8789 - loss: 0.3152 - val_F1Score: 0.6516 - val_accuracy: 0.6434 - val_loss: 1.0562\n",
      "Epoch 19/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - F1Score: 0.8912 - accuracy: 0.8904 - loss: 0.3033 - val_F1Score: 0.6490 - val_accuracy: 0.6434 - val_loss: 1.0836\n",
      "Epoch 20/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - F1Score: 0.9000 - accuracy: 0.8981 - loss: 0.2852 - val_F1Score: 0.6191 - val_accuracy: 0.6279 - val_loss: 0.9931\n",
      "Epoch 21/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - F1Score: 0.8970 - accuracy: 0.8945 - loss: 0.2780 - val_F1Score: 0.6228 - val_accuracy: 0.6279 - val_loss: 1.0246\n",
      "Epoch 22/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - F1Score: 0.9023 - accuracy: 0.9004 - loss: 0.2714 - val_F1Score: 0.6470 - val_accuracy: 0.6434 - val_loss: 1.0323\n",
      "Epoch 23/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - F1Score: 0.8978 - accuracy: 0.8963 - loss: 0.2673 - val_F1Score: 0.6202 - val_accuracy: 0.6279 - val_loss: 0.9867\n",
      "Epoch 24/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - F1Score: 0.9161 - accuracy: 0.9141 - loss: 0.2449 - val_F1Score: 0.6472 - val_accuracy: 0.6434 - val_loss: 1.0627\n",
      "Epoch 25/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - F1Score: 0.8988 - accuracy: 0.8946 - loss: 0.2695 - val_F1Score: 0.6365 - val_accuracy: 0.6512 - val_loss: 1.0455\n",
      "Epoch 26/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - F1Score: 0.9150 - accuracy: 0.9144 - loss: 0.2369 - val_F1Score: 0.6569 - val_accuracy: 0.6667 - val_loss: 1.0761\n",
      "Epoch 27/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - F1Score: 0.9106 - accuracy: 0.9088 - loss: 0.2449 - val_F1Score: 0.6339 - val_accuracy: 0.6357 - val_loss: 1.1977\n",
      "Epoch 28/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - F1Score: 0.9100 - accuracy: 0.9075 - loss: 0.2360 - val_F1Score: 0.6732 - val_accuracy: 0.6744 - val_loss: 1.0497\n",
      "Epoch 29/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - F1Score: 0.9144 - accuracy: 0.9109 - loss: 0.2378 - val_F1Score: 0.6643 - val_accuracy: 0.6744 - val_loss: 1.0026\n",
      "Epoch 30/30\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - F1Score: 0.9155 - accuracy: 0.9123 - loss: 0.2464 - val_F1Score: 0.6212 - val_accuracy: 0.6279 - val_loss: 1.1575\n"
     ]
    }
   ],
   "source": [
    "#kernel_size = \n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "model = Sequential([\n",
    "    Input(shape=(X_reshaped.shape[1], 1)),  # Explicit input layer\n",
    "    Conv1D(filters=16, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(y_onehot.shape[1], activation='softmax'),  # Output layer with softmax for multi-class\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'F1Score']\n",
    "             )\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    X_reshaped, y_onehot,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_reshaped, y_test_onehot),\n",
    "    verbose=1#,\n",
    "    #callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b27b64a-3cdc-4d98-bf3a-495745974c03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a2074b-a274-43f4-ae5b-6633cb70ac72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "3d6f826c-2bfe-48ba-b970-4dbfa7b19d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testset = test_df.drop(columns=[\"class\"])  # Features: all columns except the label\n",
    "y_testset = test_df[\"class\"]  # Class labels\n",
    "\n",
    "# Encode the labels to integers\n",
    "#label_encoder = LabelEncoder()\n",
    "y_test_encoded = label_encoder.fit_transform(y_testset)\n",
    "\n",
    "# Convert labels to one-hot encoding for categorical cross-entropy\n",
    "y_test_onehot = to_categorical(y_test_encoded)\n",
    "\n",
    "# Normalize the features\n",
    "#scaler = StandardScaler()\n",
    "X_test_scaled = scaler.fit_transform(X_testset)\n",
    "\n",
    "# Reshape the data for 1D CNN (samples, timesteps, features)\n",
    "# Timesteps = 1 since we are treating each task as one sequence\n",
    "X_test_reshaped = X_test_scaled[..., np.newaxis]  # Adding 1 channel for CNN input\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "# test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "# print(f\"Test Loss: {test_loss:.4f}\")\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d6b86f-6c3a-48b8-ba70-3e1c153a3563",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_loss, testset_accuracy, test_f1 = model.evaluate(X_test_reshaped, y_test_onehot, verbose=1)\n",
    "print(f\"Test Loss: {testset_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {testset_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {np.mean(test_f1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fd8665-3f93-456f-9508-de4f790b03ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc707242-458f-4558-ba1c-30d7f99b1fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "f3b89d36-6c35-4459-ab0f-0a65863cd80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - F1Score: 0.7041 - accuracy: 0.7069 - loss: 1.1311 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - F1Score: 0.6846 - accuracy: 0.6818 - loss: 1.3261\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - F1Score: 0.6838 - accuracy: 0.6818 - loss: 1.5765 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - F1Score: 0.7063 - accuracy: 0.7005 - loss: 1.6259 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - F1Score: 0.6969 - accuracy: 0.6918 - loss: 1.5327 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - F1Score: 0.6811 - accuracy: 0.6693 - loss: 1.9985 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - F1Score: 0.7038 - accuracy: 0.6943 - loss: 1.9488 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - F1Score: 0.7010 - accuracy: 0.6935 - loss: 1.8720 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - F1Score: 0.6973 - accuracy: 0.6896 - loss: 2.0165 \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - F1Score: 0.6827 - accuracy: 0.6836 - loss: 2.1632 \n",
      "Mean Test_set Accuracy across 10 fold cross validation: 0.6922, f1 is 0.6927462816238403\n"
     ]
    }
   ],
   "source": [
    "#cross validation\n",
    "test_set_accs = []\n",
    "test_set_f1s = []\n",
    "for i in range(10):\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test, y_test),\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Step 5: Evaluate the model\n",
    "    #test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "    #print(f\"Train Split Test Loss: {test_loss:.4f}\")\n",
    "    #print(f\"Train Split Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    testset_loss, testset_accuracy, test_f1 = model.evaluate(X_test_reshaped, y_test_onehot, verbose=1)\n",
    "    #print(f\"Test Loss: {testset_loss:.4f}\")\n",
    "    #print(f\"Test Accuracy: {testset_accuracy:.4f}\")\n",
    "    test_set_accs.append(testset_accuracy)\n",
    "    test_set_f1s.append(np.mean(test_f1))\n",
    "\n",
    "print(f\"Mean Test_set Accuracy across 10 fold cross validation: {np.mean(test_set_accs):.4f}, f1 is {np.mean(test_set_f1s)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8760866f-d922-4152-ba79-234030e2d7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1872a547-08a1-4d96-8d31-76c2252922e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"VAE_UnconGaussian_train_downsample.csv\")\n",
    "test_df = pd.read_csv(\"test_downsample.csv\")\n",
    "print(test_df.shape)\n",
    "print(test_df.isnull().values.any())\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26c1cd5-c4a3-4fbb-a4a4-7f902757bdec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
